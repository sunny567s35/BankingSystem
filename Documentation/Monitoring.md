# Kubernetes Monitoring YAML Files Analysis ðŸŒŸ

Hereâ€™s an analysis of the monitoring-related Kubernetes YAML files, along with a summary table! ðŸš€

## Analysis of Each YAML File ðŸ“œ

### 1. `prometheus-config.yaml` âš™ï¸
- **Purpose**: Defines the configuration for Prometheus to scrape metrics.
- **Key Points**:
  - Creates a ConfigMap in the `monitoring` namespace. ðŸ—‚ï¸
  - Sets a 15-second scrape interval for metrics collection. â±ï¸
  - Configures jobs to scrape Django app metrics from port 8000. ðŸ“ˆ
  - Monitors Prometheus itself on localhost:9090. ðŸ”„
  - Scrapes Redis metrics from the exporter on port 9121. ðŸ—„ï¸

### 2. `prometheus-deployment.yaml` ðŸ“Š
- **Purpose**: Deploys the Prometheus server to collect and store metrics.
- **Key Points**:
  - Runs a single replica in the `monitoring` namespace with `prom/prometheus` image. ðŸ–¼ï¸
  - Exposes port 9090 for accessing the Prometheus UI and API. ðŸšª
  - Mounts the `prometheus-config` ConfigMap as a volume for configuration. ðŸ“
  - Ensures Prometheus is always running and recoverable. ðŸ”„

### 3. `prometheus-service.yaml` ðŸŒ
- **Purpose**: Exposes Prometheus externally for access.
- **Key Points**:
  - Creates a NodePort Service in the `monitoring` namespace. ðŸ“¡
  - Maps port 9090 to node port 30090 for external access. ðŸšª
  - Targets Pods labeled `app: prometheus`. ðŸ·ï¸
  - Allows users to view metrics via a nodeâ€™s IP and port 30090. ðŸ‘€

### 4. `grafana.yaml` ðŸŽ¨
- **Purpose**: Deploys Grafana for visualizing metrics with persistent storage.
- **Key Points**:
  - Requests a 1Gi PersistentVolumeClaim for Grafana data in `monitoring`. ðŸ’¾
  - Runs a single Pod with `grafana/grafana:latest`, exposing port 3000. ðŸ–¼ï¸
  - Mounts persistent storage to save dashboards and settings. ðŸ“‚
  - Includes a NodePort Service on port 32000 for external access. ðŸ“¡

## Summary Table ðŸ“‹

| **File Name**             | **One-Line Explanation**                                                                 |
|--------------------------|-----------------------------------------------------------------------------------------|
| `prometheus-config.yaml`  | Configures Prometheus to scrape metrics from Django, Redis, and itself every 15s. âš™ï¸    |
| `prometheus-deployment.yaml`| Deploys a Prometheus server to collect metrics on port 9090. ðŸ“Š                        |
| `prometheus-service.yaml` | Exposes Prometheus externally via NodePort on 30090. ðŸŒ                                 |
| `grafana.yaml`            | Deploys Grafana with persistent storage and exposes it on NodePort 32000. ðŸŽ¨           |

---

# Kubernetes Workflow Explanation for Monitoring âš™ï¸

Hereâ€™s how Kubernetes manages the monitoring stack with Prometheus and Grafana! ðŸš€

1. **Define Desired State** ðŸ“
   - YAML files specify Prometheus and Grafana setups in the `monitoring` namespace. ðŸ“œ
   - Includes configs, deployments, services, and storage for metrics and visualization. ðŸŒ

2. **Apply Resources** ðŸš€
   - Use `kubectl apply` to send YAMLs to the Kubernetes API server. âœ…
   - The server stores these definitions and starts creating resources. ðŸ› ï¸

3. **Pod Scheduling** ðŸ—ºï¸
   - Scheduler assigns Pods (e.g., `prometheus`, `grafana`) to nodes based on resources. ðŸ“
   - Ensures availability for metrics collection and visualization. âš–ï¸

4. **Controller Management** ðŸ•¹ï¸
   - Deployment controllers keep one replica running for Prometheus and Grafana. ðŸ”„
   - Restarts any failed Pods to maintain the desired state. ðŸ›¡ï¸

5. **Service Networking** ðŸŒ
   - NodePort Services expose Prometheus on 30090 and Grafana on 32000 externally. ðŸ“¡
   - Internal configs link Prometheus to scrape targets like Django and Redis. ðŸ”—
   - Stable endpoints ensure consistent access to monitoring tools. ðŸ›¤ï¸

6. **Metrics Collection** ðŸ“ˆ
   - Prometheus scrapes metrics every 15 seconds from Django (8000), Redis (9121), and itself (9090). â±ï¸
   - Stores data for querying and analysis. ðŸ—ƒï¸
   - ConfigMap provides the scraping rules and targets. ðŸ“

7. **Visualization** ðŸŽ¨
   - Grafana connects to Prometheus to display metrics via dashboards. ðŸ‘€
   - Accessible externally on port 32000 for users to view. ðŸ“Š
   - Persistent storage keeps dashboards and settings across restarts. ðŸ’¾

8. **Self-Healing** ðŸ©º
   - If Prometheus or Grafana Pods fail, Deployments restart them. ðŸ”„
   - Services maintain access even if Pods move to new nodes. ðŸ›¡ï¸

9. **Scaling and Updates** ðŸ“
   - Scale by increasing replicas in deployment files if needed (e.g., for Grafana). ðŸ“ˆ
   - Update images (e.g., new Prometheus version) and reapply for rolling updates. ðŸ”„
  
---
```mermaid
sequenceDiagram
    participant User
    participant kubectl
    participant K8sAPI
    participant Scheduler
    participant Prometheus
    participant Grafana

    User->>kubectl: Apply Prometheus & Grafana YAMLs
    kubectl->>K8sAPI: Send definitions
    K8sAPI->>Scheduler: Schedule Pods
    Scheduler->>Prometheus: Start Prometheus Pod
    Scheduler->>Grafana: Start Grafana Pod

    Prometheus->>K8sAPI: Register scrape targets (Django, Redis, itself)
    Prometheus->>Django: Scrape /metrics on port 8000
    Prometheus->>Redis: Scrape /metrics on port 9121
    Prometheus->>Prometheus: Scrape self on port 9090

    Grafana->>Prometheus: Query metrics for dashboards
    User->>Grafana: View dashboards on NodePort 32000

    Note over Prometheus, Grafana: Pods auto-restart if failed (self-healing)
    Note over K8sAPI, Scheduler: Supports rolling updates via deployments

```
